{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scalable File System that handles the failure of nodes without data and can scale up horizontally to any number of nodes. The initial goal of HDFS was to serve large data files with high read and write performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Fault tolerance**\n",
    "- **Streaming data access**: HDFS works on a write once read many principle. HDFS does not wait for the entire file to be read before sending data to the client; instead, it sends data as soon as it reads it. The client can immediately process the received stream, which makes data processing efficient.\n",
    "- **Scalability**: Storing a huge number of small files is generally not recommended; the size of the file should be equal to or greater than the block size. Small files consume memory from name node.\n",
    "- **Simplicity**\n",
    "- **High availability**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Slow porcessing of large datasets on one computer\n",
    "    - Isolate compute and data\n",
    "    - Simultaneous processing on multiple chunks of data\n",
    "- Movement of large datasets between data nodes\n",
    "    - data replication on multiple nodes\n",
    "    - mode compute closer to data node\n",
    "- Mutliple access randomly by many users modifying files mights cause inconsistencies\n",
    "    - Only append and truncation allowed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/hdfsarchitecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture:\n",
    "- Data Plane:\n",
    "    - Data blocks\n",
    "    - Replication\n",
    "    - Checkpoints\n",
    "    - File metadata\n",
    "- Control Plane: \n",
    "    - NameNodes\n",
    "    - DataNodes\n",
    "    - JournalNodes\n",
    "    - Zookeeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Control Plane**  \n",
    "**NameNode**  \n",
    "- All data operations will first go through a NameNode and then to other relevant Hadoop components. \n",
    "- The NameNode manages the File System namespace. \n",
    "- It stores the File System tree and metadata of files and directories namely _File System namespace_, image (_fsimage_) files, and _edit logs_ files.\n",
    "\n",
    "**DataNode**  \n",
    "- They perform data block operations (creation, modification, or deletion) based on instructions that are received from NameNodes or HDFS clients. \n",
    "- They host data processing jobs such as MapReduce. \n",
    "- They report back block information to NameNodes.\n",
    "- DataNodes also communicate between each other in the case of data replication.\n",
    "\n",
    "**JournalNode**  \n",
    "- With NameNode high availability, there was a need to manage edit logs and HDFS metadata between a active and standby NameNodes. \n",
    "- JournalNodes were introduced to efficiently share edit logs and metadata between two NameNodes.\n",
    "- JournalNodes exercise concurrency write locks to ensure that edit logs are written by one active NameNode at a time.\n",
    "\n",
    "**Zookeeper**  \n",
    "- HA without automatic failover would have manual intervention to bring NameNode services back up in the event of failure. \n",
    "- Zookeeper Quorum and Zookeeper Failover controller, also known as ZKFailoverController (ZKFC). \n",
    "- Zookeeper maintains data about NameNode health and connectivity. \n",
    "- It monitors clients and notifies other clients in the event of failure. \n",
    "- Zookeeper maintains an active persistent session with each of the NameNodes, and this session is renewed by each of them upon expiry. \n",
    "- In the event of a failure or crash, the expired session is not renewed by the failed NameNode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Plane**  \n",
    "**Replication**  \n",
    "**Chunking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/hdfscommarch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NameNode keeps the complete _fsimage_ in memory so that all the metadata information requests can be served in the smallest amount of time possible and persist fsimage and edit logs on the disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# do some hdfs operations\n",
    "`hdfs dfs -mkdir /testdir`  \n",
    "`hdfs dfs -touch /testdir/somefile`  \n",
    "`hdfs dfs -copyFromLocal /etc/hosts /testdir/`  \n",
    "`hdfs dfs -cat /testdir/hosts`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Web UI**  \n",
    "`$ hdfs dfs -chown -R o+rwx /`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to fetch the fsimage in readable format  \n",
    "`hdfs dfsadmin -fetchImage /opt/hadoop`  \n",
    "\n",
    "to read the file using Offline Image Viewer tool  \n",
    "`hdfs oiv -i /opt/hadoop/fsimage_0000000000000000000 -o /opt/hadoop/fsimage_output.csv -p Delimited`\n",
    "\n",
    "to fetch edits file  \n",
    "`hdfs oev -i /tmp/hadoop-hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -o /tmp/edits-log.xml`  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before YARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MapReduce V1**\n",
    "* Single JobTracker process (One per cluster)\n",
    "    * Manages MapReduce jobs\n",
    "    * Distributes tasks to JobTrackers\n",
    "* Multiple TaskTracker processes (one per slave node)\n",
    "    * Starts and monitors MapReduce tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations**\n",
    "* Resource allocation:\n",
    "    * Slave nodes are configure with a fixed number of \"slots\" to run Map and Reduce tasks\n",
    "* Resource management process limitation:\n",
    "    * One JobTracker per cluster\n",
    "* Job types:\n",
    "    * Limited to MapReduce jobs only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter YARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* YARN supports multiple distributed processing framework:\n",
    "    * MapReduce V2\n",
    "    * Impala\n",
    "    * Spark\n",
    "    * Etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YARN Daemons**\n",
    "* Resource Manager - one per cluster\n",
    "    * Controls application startup\n",
    "    * Schedules resources on the slave nodes\n",
    "* Node Manager - one per slave node\n",
    "    * Starts all processes for a running application\n",
    "    * Manages resources on the slave nodes\n",
    "* Job History Server - one per cluster\n",
    "    * Archival of job log files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/yarn_architecture.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YARN Architecture**\n",
    "* Containners\n",
    "    * Allocation of Containers is done by the ResourceManager process\n",
    "    * Containers allocate CPU and memory on a slave node\n",
    "    * Containers run an application task\n",
    "* Application Masters\n",
    "    * Single Application Master per application\n",
    "    * Runs inside a Container\n",
    "    * Responsible for requesting additional containers on behalf of the application itself\n",
    "* Task\n",
    "    * A single user-submitted job. An applcation is broken down into multiple tasks\n",
    "    * Each task runs in a container in a Hadoop slave node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YARN Logs**\n",
    "* Multiple processes running on different nodes\n",
    "* YARN aggregates local logs from NodeManagers to central location - HDFS\n",
    "* Logs are ordered by application / job\n",
    "* Accessible on HDFS or using web UI\n",
    "* Disabled by default, usually enabled after installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Job execution with the current setup**\n",
    "In a Hadoop YARN environment, the job execution lifecycle involves several components, each playing specific roles in managing the distributed processing of data. Given your cluster setup, here's a detailed step-by-step explanation of the job execution lifecycle:\n",
    "\n",
    "### Cluster Setup\n",
    "\n",
    "- **Node 1**: NameNode\n",
    "- **Node 2**: DataNode only\n",
    "- **Node 3**: ResourceManager only\n",
    "- **Node 4**: NodeManager only\n",
    "\n",
    "### Job Execution Lifecycle\n",
    "\n",
    "1. **Job Submission**:\n",
    "   - A user or client submits a job to the YARN ResourceManager. This submission typically includes the application's JAR file, necessary resource requests (e.g., memory, CPU requirements), and configuration parameters.\n",
    "\n",
    "2. **ResourceManager Role**:\n",
    "   - The ResourceManager, running on Node 3, is responsible for resource allocation across the cluster. It forms the cornerstone of the job's lifecycle and resource scheduling.\n",
    "\n",
    "3. **ApplicationMaster Initialization**:\n",
    "   - Upon receiving a job submission, the ResourceManager negotiates resources on behalf of the job and initializes an ApplicationMaster (AM) to manage the job's execution. The ResourceManager allocates a container on Node 4 (running NodeManager) for the ApplicationMaster since it's the only node equipped to execute tasks.\n",
    "\n",
    "4. **ApplicationMaster Responsibilities**:\n",
    "   - The ApplicationMaster coordinates the execution of tasks. It requests resources (containers) from the ResourceManager based on job needs and schedules task executions. \n",
    "   - For a MapReduce job, the AM will handle splitting the job into map tasks that process data blocks and reduce tasks for aggregation.\n",
    "\n",
    "5. **Container Allocation and Execution**:\n",
    "   - The NodeManager (on Node 4) is responsible for managing the execution of tasks within the allocated containers. \n",
    "   - Task executors download the necessary code and dependencies from HDFS and execute the map/reduce tasks.\n",
    "   - Since your setup has only one DataNode (on Node 2) and one NodeManager (on Node 4), map tasks will read data remotely from Node 2, resulting in additional data transfer over the network, which could be a performance constraint.\n",
    "\n",
    "6. **DataNode Role**:\n",
    "   - Running only on Node 2, the DataNode stores the data blocks in HDFS. Although it doesn't execute tasks, it plays a critical role in providing data blocks to the map tasks initiated by the NodeManager on Node 4.\n",
    "\n",
    "7. **Task Completion and Reporting**:\n",
    "   - Once tasks are completed, the NodeManager reports success or failure of individual task attempts to the ApplicationMaster. \n",
    "   - Any failed tasks may be restarted, subject to job configuration, until they succeed or their attempts run out.\n",
    "\n",
    "8. **Job Completion and Cleanup**:\n",
    "   - Upon successful task completion and subsequent result aggregation, the ApplicationMaster conveys the job's status and final results back to the ResourceManager.\n",
    "   - The ResourceManager acknowledges the completion, and the ApplicationMaster releases its resources, wrapping up the job's lifecycle.\n",
    "   - Log aggregation (if enabled) will consolidate logs from the NodeManager to a central location on HDFS for easier access and debugging.\n",
    "\n",
    "9. **Result Access**:\n",
    "   - The results of the job are typically written back to HDFS, accessible by users or client applications for further use. The NameNode (on Node 1) manages metadata for this data, guiding future access.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exmaple**  \n",
    "\n",
    "* Create HDFS dir for logs:  \n",
    "    * `$ hdfs dfs -mkdir /app-logs`\n",
    "    * `$ hdfs dfs -chown -R 1777 /app-logs`  \n",
    "* Run a job:  \n",
    "    * From the name node run `$ hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar pi 4 10000`\n",
    "    * Switch to the YARN ResourceManager WebUI and examine the App status\n",
    "    * Switch to the name node Web UI to examine the logs \n",
    "* Check using CLI\n",
    "    * `$ yarn application -list -appStates ALL`\n",
    "    * `$ yarn logs -applicationId <application-id>`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
