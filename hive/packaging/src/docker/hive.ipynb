{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "![](./images/Apache_Hive_logo.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Apache Hive ™ is a distributed, fault-tolerant data warehouse system that enables analytics at a massive scale and facilitates reading, writing, and managing petabytes of data residing in distributed storage using SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Arch](./images/system_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **UI** – The user interface for users to submit queries and other operations to the system. As of 2011 the system had a command line interface and a web based GUI was being developed.\n",
    "* **Driver** – The component which receives the queries. This component implements the notion of session handles and provides execute and fetch APIs modeled on JDBC/ODBC interfaces.\n",
    "* **Compiler** – The component that parses the query, does semantic analysis on the different query blocks and query expressions and eventually generates an execution plan with the help of the table and partition metadata looked up from the metastore.\n",
    "* **Metastore** – The component that stores all the structure information of the various tables and partitions in the warehouse including column and column type information, the serializers and deserializers necessary to read and write data and the corresponding HDFS files where the data is stored.\n",
    "* **Execution Engine** – The component which executes the execution plan created by the compiler. The plan is a DAG of stages. The execution engine manages the dependencies between these different stages of the plan and executes these stages on the appropriate system components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Tables** – These are analogous to Tables in Relational Databases. Tables can be filtered, projected, joined and unioned. Additionally all the data of a table is stored in a directory in HDFS. Hive also supports the notion of external tables wherein a table can be created on prexisting files or directories in HDFS by providing the appropriate location to the table creation DDL. The rows in a table are organized into typed columns similar to Relational Databases.\n",
    "* **Partitions** – Each Table can have one or more partition keys which determine how the data is stored, for example a table T with a date partition column ds had files with data for a particular date stored in the <table location>/ds=<date> directory in HDFS. Partitions allow the system to prune data to be inspected based on query predicates, for example a query that is interested in rows from T that satisfy the predicate T.ds = '2008-09-01' would only have to look at files in <table location>/ds=2008-09-01/ directory in HDFS.\n",
    "* **Buckets** – Data in each partition may in turn be divided into Buckets based on the hash of a column in the table. Each bucket is stored as a file in the partition directory. Bucketing allows the system to efficiently evaluate queries that depend on a sample of data (these are queries that use the SAMPLE clause on the table).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal vs External tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Internal or Managed** tables are the default\n",
    "    * Stored in HDFS directory \n",
    "    * DROP TABLE will remove the metadata and the HDFS data\n",
    "* **External** tables are only metadata to external data in HDFS\n",
    "    * DROP TABLE will only delete the metadata and leave the actual HDFS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Build or Pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Build image (if needed)\n",
    "    * `docker build -t hive .`\n",
    "* Run the built image\n",
    "    * `$ docker run -d -p 10000:10000 -p 10002:10002 -v $(pwd)/data:/data --env SERVICE_NAME=hiveserver2 --name hive -h hive hive:4.0.0`\n",
    "* Alternatively run it from the official repo\n",
    "    * `$ docker run -d -p 10000:10000 -p 10002:10002 -v $(pwd)/data:/data --env SERVICE_NAME=hiveserver2 --name hive -h hive apache/hive:4.0.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From inside the container:  \n",
    "* `$ cd $HIVE_HOME/bin`  \n",
    "* `$ beeline -u jdbc:hive2://localhost:10000`\n",
    "* `0: jdbc:hive2://localhost:10000> show databases;`\n",
    "* `0: jdbc:hive2://localhost:10000> use default;`\n",
    "* `0: jdbc:hive2://localhost:10000> show tables;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create external table**  \n",
    "* Examine the sample csv file\n",
    "    * Attach another shell\n",
    "    * browse /data\n",
    "    * `$ head 2010-summary.csv`\n",
    "* Copy the file to HDFS\n",
    "    * `$ hdfs dfs -mkdir -p /tmp/flight_data`\n",
    "    * `$ hdfs dfs -ls /tmp`\n",
    "    * `$ hdfs dfs -copyFromLocal 2010-summary.csv /tmp/flight_data`\n",
    "* Create a database\n",
    "    * `0: jdbc:hive2://localhost:10000> create database flight_data;`\n",
    "* Create external table\n",
    "    * `0: jdbc:hive2://localhost:10000> create external table fd2010_external(SourceCountry string, DestinationCountry string, count int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '/tmp/flight_data';`\n",
    "* Verify table content\n",
    "    * `select * from fd2010_external limit 10;`\n",
    "\n",
    "**Create internal table**  \n",
    "* Create table\n",
    "    * `create table fd2010_internal(SourceCountry string, DestinationCountry string, count int);`\n",
    "    * `insert into table fd2010_internal select * from fd2010_external;`\n",
    "* Examine the HDFS structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example using ORC**  \n",
    "* Create external table\n",
    "    * `CREATE EXTERNAL TABLE IF NOT EXISTS trips (trip_id BIGINT, duration INT, start_date STRING, start_station STRING, start_terminal INT, end_date STRING, end_station STRING,  end_terminal INT, bike_number STRING, subscriber_type STRING, zip_code STRING)  STORED AS ORC LOCATION '/tmp/trips/';`\n",
    "* Create Internal table (to enable ACID)\n",
    "    * `CREATE TABLE IF NOT EXISTS inttrips (trip_id BIGINT, duration INT, start_date STRING, start_station STRING, start_terminal INT, end_date STRING, end_station STRING,  end_terminal INT, bike_number STRING, subscriber_type STRING, zip_code STRING) clustered by (trip_id) into 4 buckets  STORED AS ORC TBLPROPERTIES ('transactional'='true');`\n",
    "* Load data from external into internal table\n",
    "    * `insert into table inttrips select * from trips;`\n",
    "* Test ACID (ex. Update)\n",
    "    * `UPDATE inttrips SET duration = duration * 1.1 WHERE zip_code=\"95032\";`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
